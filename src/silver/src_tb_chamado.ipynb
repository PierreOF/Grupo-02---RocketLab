{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6215a35-eeed-41aa-8239-1326c11db165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType, StringType, BooleanType, TimestampType, IntegerType, ShortType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, regexp_replace, sha2, when, lit, uuid\n",
    "\n",
    "CATALOGO_ORIGEM = \"v_credit\"\n",
    "SCHEMA_ORIGEM = \"bronze\"\n",
    "TABELA_ORIGEM = \"chamados\"\n",
    "\n",
    "CATALOGO_DESTINO = \"v_credit\"\n",
    "SCHEMA_DESTINO = \"silver\"\n",
    "TABELA_DESTINO = \"tb_chamado\"\n",
    "TABELA_INVALIDOS_DESTINO = \"tb_chamado_invalidos\"\n",
    "\n",
    "nome_tabela_origem = f\"{CATALOGO_ORIGEM}.{SCHEMA_ORIGEM}.{TABELA_ORIGEM}\"\n",
    "nome_tabela_destino = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_DESTINO}\"\n",
    "\n",
    "timestamp_atual = F.current_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd57b19-1798-4694-af9a-189cdfa13434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DataFrame inicial (como definido no seu código)\n",
    "df_chamado = spark.table(\"v_credit.bronze.chamados\")\n",
    "\n",
    "chamado_limpo = (\n",
    "    df_chamado\n",
    "    # 1. Renomeação de Colunas\n",
    "    .withColumnRenamed(\"id_chamado\", \"cd_chamado\")\n",
    "    .withColumnRenamed(\"id_cliente\", \"cd_cliente\")\n",
    "    .withColumnRenamed(\"motivo\", \"ds_motivo\")\n",
    "    .withColumnRenamed(\"canal\", \"cd_canal\") # Novo\n",
    "    .withColumnRenamed(\"resolvido\", \"st_resolvido\")\n",
    "    .withColumnRenamed(\"tempo_espera\", \"tm_espera\") # Novo, alinhado ao metadado\n",
    "    .withColumnRenamed(\"tempo_atendimento\", \"tm_duracao\") # Novo (renomeia o antigo para o novo tm_duracao)\n",
    "    .withColumnRenamed(\"id_atendente\", \"cd_atendente\")\n",
    "    .withColumnRenamed(\"ingestion_timestamp\", \"dt_ingestion\")\n",
    "    .withColumnRenamed(\"origem\", \"dc_origem\")\n",
    "\n",
    "    .drop(\"hora_abertura_chamado\")\n",
    "    .drop(\"hora_inicio_atendimento\")\n",
    "    .drop(\"hora_finalizacao_atendimento\")\n",
    "    .drop(\"ctid_fivetran_id\")\n",
    "    .drop(\"_fivetran_deleted\")\n",
    "    .drop(\"_fivetran_synced\")\n",
    "    \n",
    "    .withColumn(\"cd_cliente\", F.col(\"cd_cliente\").cast(StringType()))\n",
    "    .withColumn(\"ds_motivo\", F.col(\"ds_motivo\").cast(StringType()))\n",
    "    \n",
    "    .withColumn(\"dt_ingestion\", F.col(\"dt_ingestion\").cast(TimestampType()))\n",
    "    .withColumn(\"dc_origem\", F.col(\"dc_origem\").cast(StringType()))\n",
    ")\n",
    "\n",
    "chamado_limpo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37ae4e6-99bb-407f-a02d-8ebf621a19c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chamado_limpo = chamado_limpo.withColumn(\n",
    "    \"st_resolvido\",\n",
    "    regexp_replace(F.col(\"st_resolvido\"), \"(?i)n.o\", \"false\")\n",
    ").withColumn(\n",
    "    \"st_resolvido\",\n",
    "    regexp_replace(F.col(\"st_resolvido\"), \"(?i)sim\", \"true\")\n",
    ").withColumn(\n",
    "    \"st_resolvido\",\n",
    "    F.col(\"st_resolvido\").cast(BooleanType())\n",
    ")\n",
    "\n",
    "chamado_limpo = chamado_limpo.withColumn(\"cd_cliente\",  F.sha2(F.col(\"cd_cliente\").cast(StringType()), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978d99e8-9ce1-4b63-a400-5f91a1470a5c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763945656163}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_log = spark.table(\"v_credit.silver.tb_chamado_log\")\n",
    "\n",
    "# 2. Realizar o Join\n",
    "# Fazemos um left join para trazer as datas do log para o chamado_limpo\n",
    "# Selecionamos apenas as colunas necessárias do log para evitar duplicidade ou ambiguidade\n",
    "chamado_limpo = chamado_limpo.join(\n",
    "    df_log.select(\"cd_chamado\", \"dh_abertura\", \"dh_inicio\", \"dh_fim\"),\n",
    "    on=\"cd_chamado\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3. Calcular tm_espera e tm_duracao e converter para bigint\n",
    "chamado_limpo = (\n",
    "    chamado_limpo\n",
    "    # tm_espera: Diferença entre início do atendimento e abertura (em segundos)\n",
    "    .withColumn(\"tm_espera\", (F.col(\"dh_inicio\").cast(\"long\") - F.col(\"dh_abertura\").cast(\"long\")))\n",
    "    \n",
    "    # tm_duracao: Diferença entre fim e início do atendimento (em segundos)\n",
    "    .withColumn(\"tm_duracao\", (F.col(\"dh_fim\").cast(\"long\") - F.col(\"dh_inicio\").cast(\"long\")))\n",
    "    \n",
    "    # Converter explicitamente para bigint (LongType)\n",
    "    .withColumn(\"tm_espera\", F.col(\"tm_espera\").cast(LongType()))\n",
    "    .withColumn(\"tm_duracao\", F.col(\"tm_duracao\").cast(LongType()))\n",
    ")\n",
    "\n",
    "display(chamado_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d469094-1a86-43ad-9d8d-34245bed32fd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763945719294}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Tratamento de caracteres inválidos (encoding) na coluna ds_motivo_rel\n",
    "# Normaliza as strings antes do join para aumentar as chances de correspondência\n",
    "chamado_limpo = (\n",
    "    chamado_limpo\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"Contrata..o\", \"Contratação\"))\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"Altera..o\", \"Alteração\"))\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"Contesta..o\", \"Contestação\"))\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"(?i)cart.o\", \"cartão\")) # (?i) para case insensitive se necessário\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"(?i)n.o\", \"não\"))\n",
    "    .withColumn(\"ds_motivo\", F.regexp_replace(F.col(\"ds_motivo\"), \"D.vidas\", \"Dúvidas\"))\n",
    ")\n",
    "\n",
    "# 2. Carregar a tabela Silver de Motivos\n",
    "tb_motivo = spark.table(\"v_credit.silver.tb_motivo\")\n",
    "\n",
    "# 3. Join corrigido usando 'ds_motivo'\n",
    "# Compara a descrição do chamado (ds_motivo_rel) com a descrição da tabela de motivos (ds_motivo)\n",
    "chamado_limpo = (\n",
    "    chamado_limpo.alias(\"c\")\n",
    "    .join(\n",
    "        tb_motivo.alias(\"m\"),\n",
    "        # Normalização (lower + trim) para garantir o match\n",
    "        F.lower(F.trim(F.col(\"c.ds_motivo\"))) == F.lower(F.trim(F.col(\"m.ds_motivo\"))),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        # Seleciona todas as colunas originais do chamado, exceto a descrição antiga\n",
    "        *[F.col(f\"c.{col}\") for col in chamado_limpo.columns if col != \"ds_motivo\"],\n",
    "        \n",
    "        # Substitui ds_motivo_rel pelo ID do motivo (cd_motivo)\n",
    "        F.col(\"m.cd_motivo\").alias(\"ds_motivo\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(chamado_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed6b21a-0f85-4737-8657-d2202c8b593f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763928618973}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "df_canais_silver = df_canais_silver.withColumnRenamed(\"cd_canal\", \"cd_canal_cadastro\")\n",
    "\n",
    "cl = chamado_limpo.alias(\"cl\")\n",
    "cs = df_canais_silver.alias(\"cs\")\n",
    "\n",
    "df_chamado_validacao = (\n",
    "    cl.join(\n",
    "        cs,\n",
    "        cl.cd_canal == cs.nm_canal,\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cd_canal_valida\",\n",
    "        when(cs.nm_canal.isNull(), lit(False)).otherwise(lit(True))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cd_canal\",\n",
    "        when(cs.nm_canal.isNull(), cl.cd_canal).otherwise(cs.cd_canal_cadastro)\n",
    "    )\n",
    "    .select(\n",
    "        *[f\"cl.{col}\" for col in chamado_limpo.columns if col != \"cd_canal\"],\n",
    "        \"cd_canal\",\n",
    "        \"cd_canal_valida\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_chamado_validacao)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src_tb_chamado",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
