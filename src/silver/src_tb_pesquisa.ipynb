{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cdff4c-2601-4912-b99b-98f893b5a0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "TABLE_SOURCE = \"v_credit.bronze.pesquisa_satisfacao\"\n",
    "TABLE_TARGET_VALIDOS = \"v_credit.silver.tb_pesquisa\"\n",
    "TABLE_TARGET_INVALIDOS = \"v_credit.silver.tb_pesquisa_invalidos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c991166e-e845-463b-9e3a-f341f0d1be8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Leitura da Tabela Bronze\n",
    "df_bronze = spark.table(TABLE_SOURCE)\n",
    "\n",
    "# 2. Identificar a Carga Mais Recente\n",
    "# Usa 'ingestion_timestamp' para encontrar o timestamp máximo da última carga.\n",
    "max_dt_ingestion = (\n",
    "    df_bronze\n",
    "    .agg(F.max(F.col(\"ingestion_timestamp\")).alias(\"max_ts\"))\n",
    "    .first()[\"max_ts\"]\n",
    ")\n",
    "\n",
    "# 3. Filtrar o DataFrame Bronze apenas pela carga mais recente\n",
    "df_filtrado = df_bronze.filter(\n",
    "    F.col(\"ingestion_timestamp\") == F.lit(max_dt_ingestion)\n",
    ")\n",
    "\n",
    "# 4. Aplicação das Transformações (usando df_filtrado)\n",
    "df_transformado = (\n",
    "    df_filtrado # <--- USANDO O DATAFRAME FILTRADO AQUI\n",
    "    .select(\n",
    "        F.col(\"id_pesquisa\").cast(\"bigint\").alias(\"cd_pesquisa\"),\n",
    "        F.col(\"id_chamado\").cast(\"bigint\").alias(\"cd_chamado\"),\n",
    "        F.when(\n",
    "            (F.col(\"nota_atendimento\").isNull()) | \n",
    "            (F.trim(F.col(\"nota_atendimento\")) == \"\"),\n",
    "            F.lit(None)\n",
    "        ).otherwise(\n",
    "            F.col(\"nota_atendimento\").cast(\"smallint\")\n",
    "        ).alias(\"nu_nota\"),\n",
    "        F.col(\"ingestion_timestamp\").alias(\"dt_ingestion\"),\n",
    "        F.coalesce(F.col(\"origem\"), F.lit(\"sistema_pesquisa\")).alias(\"dc_origem\")\n",
    "    )\n",
    "    .dropDuplicates([\"cd_pesquisa\"])\n",
    ")\n",
    "\n",
    "# O restante do seu pipeline de validação e separação permanece o mesmo\n",
    "\n",
    "df_validacao = (\n",
    "    df_transformado\n",
    "    .withColumn(\"flag_pk_valida\", F.col(\"cd_pesquisa\").isNotNull())\n",
    "    .withColumn(\"flag_fk_valida\", F.col(\"cd_chamado\").isNotNull())\n",
    "    .withColumn(\n",
    "        \"flag_nota_valida\",\n",
    "        F.col(\"nu_nota\").isNull() | \n",
    "        ((F.col(\"nu_nota\") >= 1) & (F.col(\"nu_nota\") <= 5))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"flag_qualidade\",\n",
    "        F.when(\n",
    "            F.col(\"flag_pk_valida\") &\n",
    "            F.col(\"flag_fk_valida\") &\n",
    "            F.col(\"flag_nota_valida\"),\n",
    "            F.lit(\"OK\")\n",
    "        ).otherwise(F.lit(\"ERRO\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_silver = (\n",
    "    df_validacao\n",
    "    .filter(F.col(\"flag_qualidade\") == \"OK\")\n",
    "    .drop(\"flag_pk_valida\", \"flag_fk_valida\", \"flag_nota_valida\", \"flag_qualidade\")\n",
    ")\n",
    "\n",
    "df_invalidos = df_validacao.filter(F.col(\"flag_qualidade\") == \"ERRO\")\n",
    "\n",
    "total_validos = df_silver.count()\n",
    "total_invalidos = df_invalidos.count()\n",
    "\n",
    "print(f\"✅ Validação concluída: {total_validos} válidos | {total_invalidos} inválidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce710b7-4143-47a1-8b3e-268a11e3fb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forName(spark, TABLE_TARGET_VALIDOS)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_silver.alias(\"source\"),\n",
    "    \"target.cd_pesquisa = source.cd_pesquisa\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n",
    "\n",
    "print(f\"✅ MERGE concluído\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7376252575843452,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "src_tb_pesquisa",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
