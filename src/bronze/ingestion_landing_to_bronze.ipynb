{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9f15a1-72bb-4861-915a-c9d2fa71bbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "CATALOGO_ORIGEM = \"v_credit\"\n",
    "SCHEMA_ORIGEM = \"postgres_public\"\n",
    "CATALOGO_DESTINO = \"v_credit\"\n",
    "SCHEMA_DESTINO = \"bronze\"\n",
    "ORIGEM_DADOS = \"supabase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f72eee-8198-40a5-9aff-408baa90729e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabelas_postgres = {\n",
    "    \"base_atendentes\": \"base_atendentes\",\n",
    "    \"base_motivos\": \"base_motivos\",\n",
    "    \"canais\": \"canais\",\n",
    "    \"chamados\": \"chamados\",\n",
    "    \"chamados_hora\": \"chamados_hora\",\n",
    "    \"clientes\": \"clientes\",\n",
    "    \"custos\": \"custos\",\n",
    "    \"pesquisa_satisfacao\": \"pesquisa_satisfacao\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeb3b9e-126d-4565-9949-a3ab9ea9b0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "for tabela_origem, tabela_destino in tabelas_postgres.items():\n",
    "    \n",
    "    tabela_origem_completa = f\"{CATALOGO_ORIGEM}.{SCHEMA_ORIGEM}.{tabela_origem}\"\n",
    "    tabela_destino_completa = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{tabela_destino}\"\n",
    "\n",
    "    timestamp_carga = current_timestamp() \n",
    "    \n",
    "    try:\n",
    "        print(f\"Iniciando ingestão da tabela: '{tabela_origem_completa}'\")\n",
    "        \n",
    "        df = spark.read.table(tabela_origem_completa)\n",
    "        \n",
    "        df = df.withColumn(\"ingestion_timestamp\", lit(timestamp_carga)) \n",
    "        df = df.withColumn(\"origem\", lit(ORIGEM_DADOS))\n",
    "\n",
    "        df.write.mode(\"append\").saveAsTable(tabela_destino_completa)\n",
    "        \n",
    "        print(f\"✅ Lote '{timestamp_carga}' da tabela '{tabela_destino_completa}' carregado com sucesso!\")\n",
    "        \n",
    "    except Exception as erro:\n",
    "        print(f\"❌ Erro ao processar tabela '{tabela_destino_completa}': {str(erro)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion_landing_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
