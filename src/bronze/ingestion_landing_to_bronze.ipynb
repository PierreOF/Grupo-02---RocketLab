{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9f15a1-72bb-4861-915a-c9d2fa71bbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "CATALOGO_ORIGEM = \"v_credit\"\n",
    "SCHEMA_ORIGEM = \"postgres_public\"\n",
    "CATALOGO_DESTINO = \"v_credit\"\n",
    "SCHEMA_DESTINO = \"bronze\"\n",
    "ORIGEM_DADOS = \"supabase\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Ingestao Landing → Bronze\n\n## Proposito\nEste notebook realiza a ingestao de dados da **Landing Zone** (postgres_public) para a **Camada Bronze**.\n\n## Arquitetura de Dados\n\n```\n[Supabase PostgreSQL] \n       ↓\n   [Fivetran]\n       ↓\n[Databricks Landing Zone: postgres_public]\n       ↓\n  **ESTE NOTEBOOK**\n       ↓\n[Camada Bronze: v_credit.bronze.*]\n```\n\n## O que este notebook faz:\n1. **Le dados do schema Landing** (`v_credit.postgres_public`) sincronizado pelo Fivetran\n2. **Adiciona colunas de metadata**:\n   - `ingestion_timestamp`: Timestamp da carga\n   - `origem`: Identificador da origem (sempre \"supabase\")\n3. **Salva em formato Delta** na camada Bronze com propriedades otimizadas\n\n## Decisoes Tecnicas:\n\n### Por que adicionar metadata?\n- **Rastreabilidade**: Saber quando cada registro foi carregado\n- **Auditoria**: Identificar origem dos dados\n- **Reprocessamento**: Facilita ingestao incremental futura\n\n### Por que Full Overwrite?\n- Bronze preserva historico completo da origem\n- Simplifica logica de ingestao inicial\n- Fivetran ja gerencia CDC na Landing\n\n## Tabelas processadas:\n- base_atendentes\n- base_motivos\n- canais\n- chamados\n- chamados_hora\n- clientes\n- custos\n- pesquisa_satisfacao\n\n## Dependencias:\n- DDL Bronze executado (`ddl/bronze/ddl_tabelas_bronze.ipynb`)\n- Fivetran sincronizando dados para `postgres_public`\n\n## Proximo passo:\nExecutar notebooks Silver para limpar e validar os dados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f72eee-8198-40a5-9aff-408baa90729e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabelas_postgres = {\n",
    "    \"base_atendentes\": \"base_atendentes\",\n",
    "    \"base_motivos\": \"base_motivos\",\n",
    "    \"canais\": \"canais\",\n",
    "    \"chamados\": \"chamados\",\n",
    "    \"chamados_hora\": \"chamados_hora\",\n",
    "    \"clientes\": \"clientes\",\n",
    "    \"custos\": \"custos\",\n",
    "    \"pesquisa_satisfacao\": \"pesquisa_satisfacao\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeb3b9e-126d-4565-9949-a3ab9ea9b0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "for tabela_origem, tabela_destino in tabelas_postgres.items():\n",
    "    \n",
    "    tabela_origem_completa = f\"{CATALOGO_ORIGEM}.{SCHEMA_ORIGEM}.{tabela_origem}\"\n",
    "    tabela_destino_completa = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{tabela_destino}\"\n",
    "\n",
    "    timestamp_carga = current_timestamp() \n",
    "    \n",
    "    try:\n",
    "        print(f\"Iniciando ingestão da tabela: '{tabela_origem_completa}'\")\n",
    "        \n",
    "        df = spark.read.table(tabela_origem_completa)\n",
    "        \n",
    "        df = df.withColumn(\"ingestion_timestamp\", lit(timestamp_carga)) \n",
    "        df = df.withColumn(\"origem\", lit(ORIGEM_DADOS))\n",
    "\n",
    "        df.write.mode(\"append\").saveAsTable(tabela_destino_completa)\n",
    "        \n",
    "        print(f\"✅ Lote '{timestamp_carga}' da tabela '{tabela_destino_completa}' carregado com sucesso!\")\n",
    "        \n",
    "    except Exception as erro:\n",
    "        print(f\"❌ Erro ao processar tabela '{tabela_destino_completa}': {str(erro)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion_landing_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}